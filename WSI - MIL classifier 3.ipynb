{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we repeat the same feature extraction process\n",
    "#### `only difference (WSI image width and heights divided by 2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('/root/ubc/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "      <th>image_width</th>\n",
       "      <th>image_height</th>\n",
       "      <th>is_tma</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5970</td>\n",
       "      <td>CC</td>\n",
       "      <td>27265</td>\n",
       "      <td>22900</td>\n",
       "      <td>False</td>\n",
       "      <td>5970.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64824</td>\n",
       "      <td>CC</td>\n",
       "      <td>46589</td>\n",
       "      <td>19365</td>\n",
       "      <td>False</td>\n",
       "      <td>64824.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1952</td>\n",
       "      <td>CC</td>\n",
       "      <td>33685</td>\n",
       "      <td>38053</td>\n",
       "      <td>False</td>\n",
       "      <td>1952.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59515</td>\n",
       "      <td>CC</td>\n",
       "      <td>64700</td>\n",
       "      <td>36387</td>\n",
       "      <td>False</td>\n",
       "      <td>59515.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54928</td>\n",
       "      <td>CC</td>\n",
       "      <td>36166</td>\n",
       "      <td>31487</td>\n",
       "      <td>False</td>\n",
       "      <td>54928.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>48550</td>\n",
       "      <td>MC</td>\n",
       "      <td>32431</td>\n",
       "      <td>25393</td>\n",
       "      <td>False</td>\n",
       "      <td>48550.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>39252</td>\n",
       "      <td>MC</td>\n",
       "      <td>48980</td>\n",
       "      <td>40700</td>\n",
       "      <td>False</td>\n",
       "      <td>39252.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>47431</td>\n",
       "      <td>MC</td>\n",
       "      <td>67495</td>\n",
       "      <td>46563</td>\n",
       "      <td>False</td>\n",
       "      <td>47431.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>65094</td>\n",
       "      <td>MC</td>\n",
       "      <td>55042</td>\n",
       "      <td>45080</td>\n",
       "      <td>False</td>\n",
       "      <td>65094.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>21445</td>\n",
       "      <td>MC</td>\n",
       "      <td>34200</td>\n",
       "      <td>23705</td>\n",
       "      <td>False</td>\n",
       "      <td>21445.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id label  image_width  image_height  is_tma       path\n",
       "0        5970    CC        27265         22900   False   5970.png\n",
       "1       64824    CC        46589         19365   False  64824.png\n",
       "2        1952    CC        33685         38053   False   1952.png\n",
       "3       59515    CC        64700         36387   False  59515.png\n",
       "4       54928    CC        36166         31487   False  54928.png\n",
       "..        ...   ...          ...           ...     ...        ...\n",
       "195     48550    MC        32431         25393   False  48550.png\n",
       "196     39252    MC        48980         40700   False  39252.png\n",
       "197     47431    MC        67495         46563   False  47431.png\n",
       "198     65094    MC        55042         45080   False  65094.png\n",
       "199     21445    MC        34200         23705   False  21445.png\n",
       "\n",
       "[200 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select WSI images\n",
    "non_tma_data_simple = training_data[training_data['is_tma'] == False]\n",
    "\n",
    "# Sampling 40 images from each cancer type\n",
    "balanced_data = non_tma_data_simple.groupby('label').sample(n=40, random_state=33)\n",
    "\n",
    "# Adding a new column 'path' to the balanced dataset\n",
    "balanced_data['path'] = balanced_data['image_id'].astype(str) + '.png'\n",
    "\n",
    "balanced_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Displaying the updated DataFrame\n",
    "balanced_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "additional: `img = img.resize((img.width // 2, img.height // 2))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 175 of 200: train_images/62476.png\n",
      "Processing image 176 of 200: train_images/51893.png\n",
      "Processing image 177 of 200: train_images/39872.png\n",
      "Processing image 178 of 200: train_images/14532.png\n",
      "Processing image 179 of 200: train_images/56799.png\n",
      "Processing image 180 of 200: train_images/34508.png\n",
      "Processing image 181 of 200: train_images/23523.png\n",
      "Processing image 182 of 200: train_images/28562.png\n",
      "Processing image 183 of 200: train_images/9254.png\n",
      "Processing image 184 of 200: train_images/35792.png\n",
      "Processing image 185 of 200: train_images/56993.png\n",
      "Processing image 186 of 200: train_images/20329.png\n",
      "Processing image 187 of 200: train_images/5456.png\n",
      "Processing image 188 of 200: train_images/38019.png\n",
      "Processing image 189 of 200: train_images/36678.png\n",
      "Processing image 190 of 200: train_images/25331.png\n",
      "Processing image 191 of 200: train_images/49587.png\n",
      "Processing image 192 of 200: train_images/37190.png\n",
      "Processing image 193 of 200: train_images/30986.png\n",
      "Processing image 194 of 200: train_images/3084.png\n",
      "Processing image 195 of 200: train_images/4797.png\n",
      "Processing image 196 of 200: train_images/48550.png\n",
      "Processing image 197 of 200: train_images/39252.png\n",
      "Processing image 198 of 200: train_images/47431.png\n",
      "Processing image 199 of 200: train_images/65094.png\n",
      "Processing image 200 of 200: train_images/21445.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageFile\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Assuming the ZIP file path and the root directory inside the ZIP\n",
    "zip_path = '/root/UBC-OCEAN.zip'\n",
    "zip_root_dir = 'train_images/'\n",
    "\n",
    "# Increase the maximum number of pixels PIL can process\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Load a pre-trained model for feature extraction\n",
    "model = timm.create_model('resnet101', pretrained=True, num_classes=0)\n",
    "model.eval()\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to check if the tile has tissue present\n",
    "def is_tissue_present(tile, area_threshold=0.55, low_saturation_threshold=20):   # 55% tissue\n",
    "    hsv = cv2.cvtColor(tile, cv2.COLOR_RGB2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    _, high_sat = cv2.threshold(s, low_saturation_threshold, 255, cv2.THRESH_BINARY)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    tissue_mask = cv2.dilate(high_sat, kernel, iterations=2)\n",
    "    tissue_mask = cv2.erode(tissue_mask, kernel, iterations=2)\n",
    "    tissue_ratio = np.sum(tissue_mask > 0) / (tile_size * tile_size)\n",
    "    return tissue_ratio > area_threshold\n",
    "\n",
    "# Function to extract features from a tile\n",
    "def extract_features(tile, model, transform):\n",
    "    tile = Image.fromarray(tile)\n",
    "    tile = transform(tile).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = model(tile)\n",
    "    return features.squeeze(0).numpy()\n",
    "\n",
    "# Function to process a patch of the image\n",
    "def process_patch(patch, model, transform):\n",
    "    if is_tissue_present(patch):\n",
    "        features = extract_features(patch, model, transform)\n",
    "        return features\n",
    "    return None\n",
    "\n",
    "# Define the size for the tiles\n",
    "tile_size = 512\n",
    "\n",
    "#slide_features = {}\n",
    "\n",
    "# Process each image, extract tiles, extract features, and store them\n",
    "total_images = len(balanced_data)\n",
    "for index, row in balanced_data.iterrows():\n",
    "    #if index >= 173:\n",
    "        # Stop after processing the first 50 images\n",
    "    #   continue\n",
    "\n",
    "    tile_features = [] # List to hold the features for the current image\n",
    "\n",
    "    # Extracting image from ZIP\n",
    "    image_name = row['path']  # Adjust based on your DataFrame structure\n",
    "    image_path = os.path.join(zip_root_dir, image_name)\n",
    "\n",
    "    # Print the current status\n",
    "    print(f\"Processing image {index + 1} of {total_images}: {image_path}\")\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extract(image_path, '/root/ubc_ocean/temp')\n",
    "            extracted_image_path = os.path.join('/root/ubc_ocean/temp', image_path)\n",
    "\n",
    "            with Image.open(extracted_image_path) as img:\n",
    "                img = img.resize((img.width // 2, img.height // 2))\n",
    "\n",
    "                for y in range(0, img.height, tile_size):\n",
    "                    for x in range(0, img.width, tile_size):\n",
    "                        # Read the patch\n",
    "                        patch = img.crop((x, y, x + tile_size, y + tile_size))\n",
    "                        patch = np.array(patch)  # Convert PIL Image to NumPy array\n",
    "\n",
    "                        # Process the patch\n",
    "                        features = process_patch(patch, model, transform)\n",
    "                        if features is not None:\n",
    "                            tile_features.append(features)\n",
    "\n",
    "            # Delete the extracted image to save space\n",
    "            os.remove(extracted_image_path)\n",
    "\n",
    "        # Store the extracted features and the label in the slide_features_part1 dictionary\n",
    "        slide_features[image_name] = {\n",
    "            'features': tile_features,\n",
    "            'label': row['label']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_name}: {e}\")\n",
    "    \n",
    "    import pickle\n",
    "\n",
    "    with open('/root/ubc_ocean/anar/extracted-features/half_512px_resnet101_200.pkl', 'wb') as f:\n",
    "        pickle.dump(slide_features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/ubc_ocean/anar/extracted-features/half_512px_resnet101_200.pkl', 'wb') as f:\n",
    "    pickle.dump(slide_features, f)\n",
    "\n",
    "len(slide_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train balance: {'CC': 28, 'EC': 28, 'LGSC': 28, 'HGSC': 28, 'MC': 28}\n",
      "Validation balance: {'HGSC': 4, 'CC': 4, 'LGSC': 4, 'MC': 4, 'EC': 4}\n",
      "Test balance: {'MC': 8, 'HGSC': 8, 'LGSC': 8, 'EC': 8, 'CC': 8}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# Set a fixed random state for reproducibility\n",
    "random_state = 33\n",
    "\n",
    "# Convert slide_features to a suitable format\n",
    "data = [(features['features'], features['label']) for path, features in slide_features.items()]\n",
    "\n",
    "# Organize data by labels\n",
    "data_by_label = defaultdict(list)\n",
    "for features, label in data:\n",
    "    data_by_label[label].append((features, label))\n",
    "\n",
    "# Split data for each label into train, validation, and test\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "for label, label_data in data_by_label.items():\n",
    "    # Split data for this label into train and test with a fixed random state\n",
    "    train_val_label_data, test_label_data = train_test_split(label_data, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # Split train data into train and validation with a fixed random state\n",
    "    train_label_data, val_label_data = train_test_split(train_val_label_data, test_size=0.125, random_state=random_state)  # 0.25 x 0.8 = 0.2 of original\n",
    "    \n",
    "    # Append split data to respective sets\n",
    "    train_data.extend(train_label_data)\n",
    "    val_data.extend(val_label_data)\n",
    "    test_data.extend(test_label_data)\n",
    "\n",
    "# Shuffle the datasets\n",
    "random.seed(random_state)\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(val_data)\n",
    "random.shuffle(test_data)\n",
    "\n",
    "# Function to check balance in each set\n",
    "def check_balance(dataset):\n",
    "    label_counts = defaultdict(int)\n",
    "    for _, label in dataset:\n",
    "        label_counts[label] += 1\n",
    "    return dict(label_counts)\n",
    "\n",
    "# Display balance of each set\n",
    "print(\"Train balance:\", check_balance(train_data))\n",
    "print(\"Validation balance:\", check_balance(val_data))\n",
    "print(\"Test balance:\", check_balance(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from label strings to integers\n",
    "unique_labels = sorted(set(label for _, label in data))\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "class MILDataset(Dataset):\n",
    "    def __init__(self, data, label_to_idx):\n",
    "        self.data = data\n",
    "        self.label_to_idx = label_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature_vectors, label = self.data[idx]\n",
    "        label_idx = self.label_to_idx[label]  # Convert label to integer\n",
    "        return torch.tensor(feature_vectors), torch.tensor(label_idx, dtype=torch.float32)\n",
    "\n",
    "# Create Datasets for train, validation, and test\n",
    "train_dataset = MILDataset(train_data, label_to_idx)\n",
    "val_dataset = MILDataset(val_data, label_to_idx)\n",
    "test_dataset = MILDataset(test_data, label_to_idx)\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93202/1868705955.py:16: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  return torch.tensor(feature_vectors), torch.tensor(label_idx, dtype=torch.float32)\n",
      "/root/.conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Train Loss: 1.6396, Train Acc: 20.71%, Validation Loss: 1.6049, Val Acc: 20.00%\n",
      "Epoch 2/15, Train Loss: 1.5628, Train Acc: 25.00%, Validation Loss: 1.5783, Val Acc: 25.00%\n",
      "Epoch 3/15, Train Loss: 1.4579, Train Acc: 37.86%, Validation Loss: 1.5405, Val Acc: 30.00%\n",
      "Epoch 4/15, Train Loss: 1.2339, Train Acc: 52.14%, Validation Loss: 1.5252, Val Acc: 35.00%\n",
      "Epoch 5/15, Train Loss: 1.1075, Train Acc: 54.29%, Validation Loss: 1.3934, Val Acc: 45.00%\n",
      "Epoch 6/15, Train Loss: 0.9177, Train Acc: 62.86%, Validation Loss: 1.3636, Val Acc: 40.00%\n",
      "Epoch 7/15, Train Loss: 0.7558, Train Acc: 73.57%, Validation Loss: 1.3521, Val Acc: 35.00%\n",
      "Epoch 8/15, Train Loss: 0.6362, Train Acc: 75.00%, Validation Loss: 1.2380, Val Acc: 55.00%\n",
      "Epoch 9/15, Train Loss: 0.5334, Train Acc: 80.71%, Validation Loss: 1.4628, Val Acc: 45.00%\n",
      "Epoch 10/15, Train Loss: 0.4399, Train Acc: 85.00%, Validation Loss: 1.4080, Val Acc: 50.00%\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 11/15, Train Loss: 0.3557, Train Acc: 89.29%, Validation Loss: 1.8737, Val Acc: 45.00%\n",
      "Stopping early due to no improvement in validation loss.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "class AttentionMIL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(AttentionMIL, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Softmax(dim=0)\n",
    "        )\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)  # num_classes instead of 1\n",
    "\n",
    "    def forward(self, bag):\n",
    "        h = torch.relu(self.fc1(bag))\n",
    "        a = self.attention(h)\n",
    "        v = torch.sum(a * h, dim=0)\n",
    "        y = self.classifier(v)  # Remove softmax here; output raw scores\n",
    "        return y, a\n",
    "\n",
    "# Number of unique classes\n",
    "num_classes = len(unique_labels)\n",
    "\n",
    "model = AttentionMIL(input_dim=2048, hidden_dim=256, num_classes=num_classes)\n",
    "loss_function = nn.CrossEntropyLoss()  # CrossEntropyLoss for multiclass\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Early Stopping Parameters\n",
    "best_val_loss = float('inf')\n",
    "patience = 4\n",
    "patience_counter = 0\n",
    "\n",
    "# Model Training with Validation\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    # Training loop\n",
    "    for bags, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        bags = bags.squeeze(0)  # Remove the extra dimension from bags\n",
    "        labels = labels.squeeze(0).long()  # Remove extra dimension and ensure long type for labels\n",
    "        output, _ = model(bags)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 0)\n",
    "        train_total += 1\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for bags, labels in val_loader:\n",
    "            bags = bags.squeeze(0)\n",
    "            labels = labels.squeeze(0).long()\n",
    "            output, _ = model(bags)\n",
    "            loss = loss_function(output, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 0)\n",
    "            val_total += 1\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Stopping early due to no improvement in validation loss.\")\n",
    "        break\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Validation Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7000\n",
      "Precision: 0.7062\n",
      "Recall: 0.7000\n",
      "F1 Score: 0.6948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bags, labels in test_loader:\n",
    "        output, _ = model(bags.squeeze(0))\n",
    "        _, predicted_labels = torch.max(output, 0)  # Get the index of the max log-probability\n",
    "        predictions.append(predicted_labels.item())  # Append scalar value\n",
    "        true_labels.append(labels.squeeze(0).item())  # Append scalar value\n",
    "\n",
    "# Convert lists to arrays for metric calculation\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='macro', zero_division=1)\n",
    "recall = recall_score(true_labels, predictions, average='macro')\n",
    "f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MC', 'LGSC', 'EC', 'LGSC', 'LGSC', 'HGSC', 'HGSC', 'LGSC', 'HGSC', 'HGSC', 'EC', 'EC', 'LGSC', 'LGSC', 'MC', 'LGSC', 'CC', 'HGSC', 'EC', 'CC', 'CC', 'HGSC', 'CC', 'LGSC', 'MC', 'LGSC', 'MC', 'EC', 'HGSC', 'MC', 'MC', 'CC', 'CC', 'LGSC', 'MC', 'EC', 'HGSC', 'MC', 'LGSC', 'EC']\n",
      "['MC', 'HGSC', 'HGSC', 'LGSC', 'LGSC', 'HGSC', 'EC', 'LGSC', 'HGSC', 'HGSC', 'EC', 'MC', 'LGSC', 'LGSC', 'EC', 'EC', 'CC', 'EC', 'EC', 'CC', 'CC', 'HGSC', 'CC', 'HGSC', 'MC', 'LGSC', 'MC', 'EC', 'CC', 'MC', 'MC', 'CC', 'CC', 'LGSC', 'EC', 'CC', 'HGSC', 'MC', 'LGSC', 'MC']\n"
     ]
    }
   ],
   "source": [
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}\n",
    "\n",
    "# Use idx_to_label to map numeric predictions back to label names\n",
    "predicted_labels = [idx_to_label[int(idx)] for idx in predictions]\n",
    "true_label_names = [idx_to_label[int(idx)] for idx in true_labels]\n",
    "\n",
    "# Now predicted_labels and true_label_names contain the label names\n",
    "print(predicted_labels)\n",
    "print(true_label_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
